{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Automated Q&A.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNlAebluCcBJQCdOyh9mtRK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":537},"id":"d1lYlJ6oOU5m","executionInfo":{"status":"error","timestamp":1648918943285,"user_tz":-330,"elapsed":4729,"user":{"displayName":"Bharath P","userId":"03984529975862912922"}},"outputId":"6ba7622c-f7bd-4989-af6d-6c18cb996c88"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-764c96cb64d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-764c96cb64d4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mfout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jsondata.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mquesfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your filename : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.corpus import stopwords\n","import spacy\n","import glob\n","import nltk\n","from nltk import tokenize\n","import sys\n","import numpy as np\n","from nltk.corpus import wordnet\n","from collections import defaultdict\n","from nltk.tag import pos_tag\n","import string\n","import json\n","import pprint\n","\n","\n","#get all filenames\n","#print (filenames)\n","\n","nlp = spacy.load('en_core_web_sm')\n","class QuestionAnswerModule:\n","\tfilenames = glob.glob(\"WikipediaArticles/*.txt\")\n","\n","\t#read file contents and store it in format [\"content1\", \"content2\"]\n","\tdef readfiles(self):\n","\t\tcorpora = []\n","\t\tcontent= \"\"\n","\t\tfor fname in self.filenames:\n","\t\t\t#print(fname)\n","\t\t\t#file = open(fname,'r',encoding='utf-8-sig')\n","\t\t\t#for line in file:\n","\t\t\t#\tcontent += line.rstrip('\\n')\n","\t\t\tcontent = open(fname,'r',encoding='utf-8-sig').read()\n","\t\t\t#print (content)\n","\t\t\tcorpora.append(content)\n","\t\treturn corpora\n","\n","\t#make question as your last document and calculate tf-idf vectors\n","\tdef tf_idf(self,corpora):\n","\t\tvectorizer = TfidfVectorizer()\n","\t\ttf_idfmatrix = vectorizer.fit_transform(corpora)\n","\t\tnp.set_printoptions(threshold=sys.maxsize)\n","\t\t#print(tf_idfmatrix.shape)\n","\t\t#vector = vector.toarray()\n","\t\treturn tf_idfmatrix\n","\n","\t#find cosine similarity of question with documents\n","\tdef cosine_sim(self, vector):\n","\t\tcos_array = cosine_similarity(vector[-1:],vector)\n","\t\t#print(cos_array)\n","\t\treturn cos_array\n","\n","\t#return top k documents for the question\n","\tdef get_top_k(self,cos_array, k):\n","\t\t#return top 3 documents\n","\t\tflat = cos_array.flatten()\n","\t\tind = np.argpartition(flat,-k)[-k:]\n","\t\tind = ind[np.argsort(-flat[ind])]\n","\t\t#print (ind)\n","\t\tind = ind[1:]\n","\t\t#print(self.filenames[ind[1]],self.filenames[ind[2]],self.filenames[ind[3]])\n","\t\treturn ind\n","\n","\t#do dependency parsing on the question and store words except stop words\n","\t# in the form [(word, dependecy parse tag, pos tag)]\n","\tdef dep_parse_ques(self,question,ques_types):\n","\t\tsearch_list = []\n","\t\tdoc = nlp(question)\n","\t\troot = \"\"\n","\t\tnsub = \"\"\n","\t\tbackroot = \"\"\n","\t\tverb = \"\"\n","\t\tfor token in doc:\n","\t\t\tif token.dep_ == \"ROOT\":\n","\t\t\t\tbackroot = token.text\n","\t\t\tif token.text.lower() not in stopwords.words('english') and token.text.lower() not in ques_types:\n","\t\t\t#if token.text.lower() not in ques_types:\t\n","\t\t\t\t#print(token.text, token.pos_, token.dep_)\n","\t\t\t\tif token.dep_ in (\"ROOT\",\"acl\",\"advcl\",\"amod\",\"advmod\",\"compound\",\"csubj\",\"nsubjpass\",\n","\t\t\t\t \t\"nn\",\"attr\",\"dobj\",\"npmod\",\"nsubj\",\"pobj\",\"acomp\",\"pcomp\",\"relcl\"):\n","\t\t\t\t\t#print(token.text, token.pos_, token.dep_)\n","\t\t\t\t\t#if token.dep_ in (\"ROOT\") or token.pos_ in (\"VERB\"):\n","\t\t\t\t\tsearch_list.append([token.text.lower(),token.dep_,token.pos_])\n","\t\t\t\t\tif(token.dep_ == \"ROOT\"):\n","\t\t\t\t\t\troot = token.text\n","\t\t\t\t\tif(token.dep_ == \"nsubj\"):\n","\t\t\t\t\t\tnsub = token.text\n","\t\t\t\t\tif(token.pos_ == \"VERB\"):\n","\t\t\t\t\t\tverb = token.text\n","\t\tif root == \"\" and nsub != \"\":\n","\t\t\troot = nsub\n","\t\telif root == \"\" and nsub == \"\" and verb!=\"\":\n","\t\t\troot = verb\n","\t\telse:\n","\t\t\troot = backroot\n","\t\t#print(\"              \",root)\n","\t\treturn root,search_list\n","\n","\t#extract sentences from the top 3 documents based on root and its synonyms\n","\t'''def extract_sentences_root(self, ques, search_dict,top_indices):\n","\t\tsentences_set = []\n","\t\tques_v = []\n","\t\t#print (ques)\n","\t\tfor word in ques.split(\" \"):\n","\t\t\tif word:\n","\t\t\t\tques_v.append(word)\n","\t\t#print(ques_v)\n","\t\tfor t in top_indices:\n","\t\t\tfile = self.filenames[t]\n","\t\t\twith open(file,'r',encoding='utf-8-sig') as fp:\n","\t\t\t\tcontent = fp.read()\n","\t\t\t\tcontent = tokenize.sent_tokenize(content)\n","\t\t\t\tfor line in content:\n","\t\t\t\t\t#print(line)\n","\t\t\t\t\t#if any of the words in search list are in the line just read then\n","\t\t\t\t\tnewline = nlp(line)\n","\t\t\t\t\tline_v = []\n","\t\t\t\t\tfor word in newline:\n","\t\t\t\t\t\tline_v.append(word.lemma_)\n","\t\t\t\t\ttoken = []\n","\t\t\t\t\tfor key, value in search_dict.items():\n","\t\t\t\t\t\tkey = nlp(key)\n","\t\t\t\t\t\tfor t in key:\n","\t\t\t\t\t\t\ttoken.append(t)\n","\t\t\t\t\t\t\ttoken.append(t.lemma_)\n","\t\t\t\t\t\tif any(str(word) in line_v for word in token):\n","\t\t\t\t\t\t\t#print (\"found \", line)\n","\t\t\t\t\t\t\tsentences_set.append(line)\n","\t\t\t\t\tfor key, value in search_dict.items():\n","\t\t\t\t\t\tfor val in value:\n","\t\t\t\t\t\t\tif val in line_v and any(word in line_v for word in ques_v):\n","\t\t\t\t\t\t\t\t#print(\"word:\",val,\" \",line)\n","\t\t\t\t\t\t\t\tsentences_set.append(line)\n","\t\t\t\t\t#sentences_set.append(line)\n","\t\t#sentences_set = set(sentences_set)\n","\t\treturn sentences_set'''\n","\n","\tdef check_ques_type(self, question,sorted_overlapped):\n","\t\t#print('Entered', question,sorted_overlapped[0][0])\n","\t\tif any(word in question.lower() for word in ['who','whom']):\n","\t\t\tfiltered = self.extract_sent_named_entity('who', sorted_overlapped)\n","\t\telif 'when' in question.lower():\n","\t\t\tfiltered = self.extract_sent_named_entity('when', sorted_overlapped)\n","\t\telse:\n","\t\t\tfiltered = self.extract_sent_named_entity('where', sorted_overlapped)\n","\t\treturn filtered\n","\n","\tdef extract_sent_named_entity(self, question, sorted_overlapped):\n","\t\t#print('Entered entity function', question, sorted_overlapped[0][0])\n","\t\tent_type = []\n","\t\tnlp = spacy.load(\"en_core_web_sm\")\n","\t\tif question == 'who':\n","\t\t\t#print(\"Entered who type\\n\")\n","\t\t\tfor sent in sorted_overlapped:\n","\t\t\t\t#print(sent[0])\n","\t\t\t\tans = []\n","\t\t\t\tflag = 0\n","\t\t\t\tdoc = nlp(sent[0])\n","\t\t\t\tfor ent in doc.ents:\n","\t\t\t\t\tif (ent.label_ == \"PERSON\") or (ent.label_== \"ORG\"):   #Change here based on desired entity\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\t\t\t\t\tflag = 1\n","\t\t\t\t\t#print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","\t\t\t\tif (flag ==1):\n","\t\t\t\t\tent_type.append((sent[0],set(ans)))\n","\n","\t\telif question == 'where':\n","\t\t\t#print(\"Entered where type\\n\")\n","\t\t\tfor sent in sorted_overlapped:\n","\t\t\t\t#print(sent[0])\n","\t\t\t\tans = []\n","\t\t\t\tflag = 0\n","\t\t\t\tdoc = nlp(sent[0])\n","\t\t\t\tfor ent in doc.ents:\n","\t\t\t\t\tif (ent.label_ == \"LOC\") or (ent.label_ == \"GPE\"):      #Change here based on desired entity\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\t\t\t\t\tflag = 1\n","\t\t\t\t\t#print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","\t\t\t\tif (flag ==1):\n","\t\t\t\t\tent_type.append((sent[0],set(ans)))\n","\t\t\t\t\t\n","\t\telse: #type == when\n","\t\t\t#print(\"Entered when type\\n\")\n","\t\t\tfor sent in sorted_overlapped:\n","\t\t\t\t#print(sent[0])\n","\t\t\t\tans = []\n","\t\t\t\tflag = 0\n","\t\t\t\tdoc = nlp(sent[0])\n","\t\t\t\tfor ent in doc.ents:\n","\t\t\t\t\tif (ent.label_ == \"DATE\") or (ent.label_== \"TIME\"):     #Change here based on desired entity\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\t\t\t\t\tflag = 1\n","\t\t\t\t\t#print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","\t\t\t\tif (flag ==1):\n","\t\t\t\t\tent_type.append((sent[0],set(ans)))\n","\t\treturn ent_type\n","\n","\t# find synonyms of the words in the list\n","\tdef extract_syn(self, search_list):\n","\t\tsyno = {}\n","\t\thab = []\n","\t\tkan = []\n","\t\tflat_list2 = []\n","\t\tfor list_ in search_list:\n","\t\t\twordp = nltk.word_tokenize(list_[0])\n","\t\t\ttagged_senta = pos_tag(wordp)\n","\t\t\tword = list_[0]\n","\t\t\tfor wo, pos in tagged_senta:\n","\t\t\t\tif list_[2] != 'PROPN':\n","\t\t\t\t\t#*************SYNONYMS*******************\n","\t\t\t\t\tfor syn in wordnet.synsets(word):\n","\t\t\t\t\t\t#print(syn)\n","\t\t\t\t\t\tfor l in syn.lemmas():\n","\t\t\t\t\t\t\t#print(l)\n","\t\t\t\t\t\t\tif word not in syno:\n","\t\t\t\t\t\t\t\tsyno[word] = [l.name(),word]\n","\t\t\t\t\t\t\telse:\n","\t\t\t\t\t\t\t\tsyno[word].append(l.name())\n","\t\t\t\t\t\t\t        \n","\t\t\t\t\t#***********HYPONYMS********************\n","\t\t\t\t\t'''for i in range(0,len(wordnet.synsets(word))):\n","\t\t\t\t\t\tabc = wordnet.synset(wordnet.synsets(word)[i].name()).hyponyms()\n","\t\t\t\t\t\tfor j in range(len(abc)):\n","\t\t\t\t\t\t\thab.append(abc[j].lemma_names())\n","\t\t\t\t\tflat_list2 = [item for sublist in hab for item in sublist]\n","\t\t\t\t\thab.clear()\n","\t\t\t\t\t#print(flat_list2)\n","\t\t\t\t\tfor hypo in flat_list2: \n","\t\t\t\t\t\tsyno[word].append(hypo)\n","\t\t\t\t\tflat_list2.clear()'''\n","\t\t                \n","\t\t\t\t\t#***********HYPERNYMS********************  \n","\t\t\t\t\tfor i in range(0,len(wordnet.synsets(word))): \n","\t\t\t\t\t\txyz = wordnet.synset(wordnet.synsets(word)[i].name()).hypernyms() \n","\t\t\t\t\t\tfor h in range(len(xyz)): \n","\t\t\t\t\t\t\tkan.append(xyz[h].lemma_names()) \n","\t\t\t\t\tflat_list1 = [item for sublist in kan for item in sublist] \n","\t\t\t\t\tkan.clear()\n","\t\t\t\t\tfor hyper in flat_list1: \n","\t\t\t\t\t\tsyno[word].append(hyper) \n","\t\t\t\t\tflat_list1.clear()\n","\t\tfor key, value in syno.items():\n","\t\t\tsyno[key] = set(value)\n","\t\treturn syno\n","\n","\tdef overlap(self, top_indices, ques):\n","\t\tsentences_set = []\n","\t\tques_v = []\n","\t\tques = nlp(ques)\n","\t\tfor word in ques:\n","\t\t\tif word:\n","\t\t\t\tques_v.append(word.lemma_)\n","\t\t#print(ques_v)\n","\t\tfor t in top_indices:\n","\t\t\tfile = self.filenames[t]\n","\t\t\t#print(\"#######################\",file)\n","\t\t\twith open(file,'r',encoding='utf-8-sig') as fp:\n","\t\t\t\tcontent = fp.read()\n","\t\t\t\tcontent = tokenize.sent_tokenize(content)\n","\t\t\t\tfor line in content:\n","\t\t\t\t\tif line.startswith('See also'):\n","\t\t\t\t\t\tbreak\n","\t\t\t\t\tnewline = nlp(line)\n","\t\t\t\t\tline_v = []\n","\t\t\t\t\tfor word in newline:\n","\t\t\t\t\t\tline_v.append(word.lemma_)\n","\t\t\t\t\tsentences_set.append((line,len(list(set(ques_v).intersection(set(line_v))))))\n","\t\treturn sentences_set,file\n","\n","\tdef Sort_Tuple(self,tup):  \n","\t\ttup.sort(key = lambda x: x[1],reverse = True)  \n","\t\treturn tup\n","\n","\tdef generateJson(self,f,question,answer,sentences,documents):\n","\t\tjsonData = dict({})\n","\t\tjsonData[\"Question\"] = question\n","\t\tjsonData[\"answers\"] = dict({})\n","\t\tansDict = dict()\n","\t\ti = 0\n","\t\tansDict[str(i+1)] = answer\n","\t\tjsonData[\"answers\"] = ansDict\n","\n","\t\ti = 0\n","\t\tjsonData[\"sentences\"] = dict({})\n","\t\tsentDict = dict()\n","\t\tsentDict[str(i+1)] = sentences\n","\t\tjsonData[\"sentences\"] = sentDict\n","\n","\t\tjsonData[\"documents\"] = dict({})\n","\t\tdocDict = dict()\n","\t\ti = 0\n","\t\tdocDict[str(i+1)] = documents.split(\"\\\\\")[1]\n","\t\tjsonData[\"documents\"] = docDict\n","\n","\t\tjsonData = [jsonData]\n","\t\tdata = json.dumps(jsonData)\n","\n","\n","\t\t#print(jsonData)\n","\t\t#print(\"data\")\n","\t\t#print(data)\n","\t\tf.write(data)\n","\t\tf.write(\"\\n\")\n","  \n"," \t# get filename where ans is located\n","\tdef getfilename(self,index):\n","\t\t#print(self.filenames[index[0]])\n","\t\treturn self.filenames[index[0]]\n","\n","\t# dependency parse on sentences : root of sentence should equal root of question\n","\t# proper noun of question should be in the sentence\n","\tdef dependency_parse(self,results,root,quesnoun,syn_list):\n","\t\trootfiltered = []\n","\t\t#print (quesnoun)\n","\t\tfor sent in results:\n","\t\t\tsentence = nlp(sent)\n","\t\t\tsentroots = []\n","\t\t\tnounlist = []\n","\t\t\tfor token in sentence:\n","\t\t\t\tif token.dep_ == 'ROOT':\n","\t\t\t\t\tsentroots.append(token.lemma_.lower())\n","\t\t\t#if str(root[0]) in roots or str(root[0].lemma_) in roots:\n","\t\t\t#print(str(root[0]))\n","\t\t\tif str(root[0]) in syn_list:\n","\t\t\t\tfor value in syn_list[str(root[0])]:\n","\t\t\t\t\tif value.lower() in sentroots or str(root[0].lemma_).lower() in sentroots:\n","\t\t\t\t\t\t#print(\"yes \",sentroots,\" yes \",sent[0])\n","\t\t\t\t\t\trootfiltered.append(sent)\n","\t\t\t\t\t\tbreak\n","\t\t\telse:\n","\t\t\t\tif str(root[0].lemma_).lower() in sentroots:\n","\t\t\t\t\t\t#print(\"yes \",sentroots,\" yes \",sent[0])\n","\t\t\t\t\trootfiltered.append(sent)\n","\t\t\t\t\tbreak\n","\n","\t\tnounfiltered = []\n","\t\tfor sent in rootfiltered:\n","\t\t\tsentence = nlp(sent)\n","\t\t\tnounlist = []\n","\t\t\tfor token in sentence:\n","\t\t\t\tif token.dep_ in ('nsubj','dobj','compound','nsubjpass'):\n","\t\t\t\t\tnounlist.append(token.text.lower())\n","\t\t\t\t\t#print(nounlist)\n","\t\t\tfor noun in quesnoun:\n","\t\t\t\tif noun.lower() in nounlist:\n","\t\t\t\t\t#print(\"no\",sent[0])\n","\t\t\t\t\t#print(nounlist,\" yes \",quesnoun,sent[0])\n","\t\t\t\t\tnounfiltered.append(sent)\n","\t\t\t\t\tbreak\n","\t\t#print(nounfiltered[0:2])\n","\t\treturn nounfiltered\n","\t\t\t\n","\t# get exact answer from the answer sentence \n","\tdef extract_ans(self,question,answer):\n","\t\tdoc = nlp(answer)\n","\t\tans = []\n","\t\tif any(word in question.lower() for word in ['who','whom']):\n","\t\t\tfor ent in doc.ents:\n","\t\t\t\tif (ent.label_ == \"PERSON\") or (ent.label_== \"ORG\"): \n","\t\t\t\t\tif ent.text not in question:\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\telif 'where' in question.lower():\n","\t\t\tfor ent in doc.ents:\n","\t\t\t\tif (ent.label_ == \"LOC\") or (ent.label_ == \"GPE\"):      #Change here based on desired entity\n","\t\t\t\t\tif ent.text not in question:\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\telse:\n","\t\t\tfor ent in doc.ents:\n","\t\t\t\tif (ent.label_ == \"DATE\") or (ent.label_== \"TIME\"):       #Change here based on desired entity\n","\t\t\t\t\tif ent.text not in question:\n","\t\t\t\t\t\tans.append(ent.text)\n","\t\treturn ans\n","\n","\n","def main():\n","\tfout = open(\"jsondata.txt\",\"w+\")\n","\tquesfile = input(\"Enter your filename : \")\n","\twith open(quesfile,'r',encoding='utf-8-sig') as fp:\n","\t\tquestion = fp.readline().rstrip()\n","\t\twhile question:\n","\t\t\t#print(question)\n","\t\t\t#question = input(\"Enter question: \")\n","\t\t\tquestion.replace(\"?\", \"\")\n","\t\t\tques_types = ['who','whom','when','where']\t\t\t\t\t\t\t#types of questions we are handling\n","\t\t\tob = QuestionAnswerModule() \n","\t\t\tdoc = nlp(question)\n","\t\t\tques = \"\"\n","\t\t\tfor token in doc:\n","\t\t\t\tif token.text.lower() not in stopwords.words('english') and token.text.lower() not in ques_types:\n","\t\t\t\t\tques+=token.text+\" \"\n","\t\t\t#print(ques)\n","\n","\t\t\t# ********read files and get most relevant document*******\n","\t\t\tcorpora = ob.readfiles()\n","\t\t\tcorpora.append(ques)\n","\t\t\tvector = ob.tf_idf(corpora)\t\t\t\t\t\t\n","\t\t\tcos_array = ob.cosine_sim(vector)\n","\t\t\ttop_indices = ob.get_top_k(cos_array, 2)\t\t\t\t\t\t\t# get indices of top 2 documents\n","\t\t\t\n","\t\t\t# ******** parse the question and get synonyms of non Proper Noun words ********\n","\t\t\troot, ques_search_list = ob.dep_parse_ques(question,ques_types)\t\t#parse question into word, dependency parse tag\n","\t\t\t#print (ques_search_list)\n","\t\t\tsyn_list = ob.extract_syn(ques_search_list)\t\t\t\t\t\t\t\t\t# get synonyms of the root verb\n","\t\t\t#print(syn_list)\n","\t\t\t\n","\t\t\t\n","\t\t\t# ********* find top 20 overlapped sentences with question ***********\n","\t\t\ts = \"\"\n","\t\t\tfor word in question.split(\" \"):\n","\t\t\t\tif word.lower() not in ques_types and word.lower() not in stopwords.words('english'):\n","\t\t\t\t\ts += word + \" \"\n","\t\t\tfor key,value in syn_list.items():\n","\t\t\t\tfor val in value:\n","\t\t\t\t\ts+=val + \" \"\n","\t\t\t#print(s)\n","\t\t\toverlap_sent,filename = ob.overlap(top_indices, s)\n","\t\t\tsorted_overlapped = ob.Sort_Tuple(overlap_sent)[0:20]\n","\n","\t\t\t# ********* do named entity on the overlapped sentences ************\n","\t\t\tfiltered_res = ob.check_ques_type(question,sorted_overlapped)\n","\t\t\t#print (filtered_res[0:5])\n","\n","\t\t\t# ******** tf idf of sentences and question **********************\n","\t\t\tnew_ques = []\n","\t\t\tfor sentence in filtered_res:\n","\t\t\t\tnew_ques.append(sentence[0])\n","\t\t\ts = \"\"\n","\t\t\tfor word in question.split(\" \"):\n","\t\t\t\tif word.lower() not in ques_types and word.lower() not in stopwords.words('english'):\n","\t\t\t\t\ts += word + \" \"\n","\t\t\tfor key,value in syn_list.items():\n","\t\t\t\tk = nlp(key)\n","\t\t\t\tif k[0].pos_ == \"VERB\":\n","\t\t\t\t\tfor val in value:\n","\t\t\t\t\t\ts+=val + \" \"\n","\t\t\tnew_ques.append(s.lower())\n","\t\t\tvector = ob.tf_idf(new_ques)\n","\t\t\tcos_array = ob.cosine_sim(vector)\n","\t\t\ttop_indices = ob.get_top_k(cos_array, 5)\t\t\t\t# get only top 5 sentences\n","\t\t\tf = cos_array.flatten()\n","\t\t\ttf_idf_result = []\n","\t\t\tfor index in top_indices:\n","\t\t\t \t#print (\"index:\",index,\" \",\"cosine:\",f[index],\" \",new_ques[index])\n","\t\t\t \ttf_idf_result.append(new_ques[index])\n","\n","\t\t\t# ********* do dependency parse on the sentences ********\n","\t\t\troot = nlp(root)\n","\t\t\tquesnoun = []\n","\t\t\tfor t in ques_search_list:\n","\t\t\t\tif t[2] == 'PROPN':\n","\t\t\t\t\tquesnoun.append(t[0])\n","\t\t\t#print (quesnoun)\n","\t\t\tfinal_res = ob.dependency_parse(tf_idf_result,root,quesnoun,syn_list)\n","\t\t\t#print (final_res)\n","\n","\t\t\t# check if dependency parse returned 0 then ans is first sentence of named entity set of sentences *****\n","\t\t\tif len(final_res) == 0:\n","\t\t\t\tans_sent = filtered_res[0][0]\n","\t\t\telse:\n","\t\t\t\tans_sent = final_res[0]\n","\n","\t\t\tanslist = ob.extract_ans(question, ans_sent)\n","\t\t\tanslist = set(anslist)\n","\t\t\tanswer = \",\".join(anslist)\n","\t\t\tprint(\"final ans \", answer)\n","\n","\t\t\t# ******* save in file ******\n","\t\t\t#filename = ob.getfilename(top_indices)\n","\t\t\tob.generateJson(fout,question,answer,ans_sent,filename)\n","\t\t\tquestion = fp.readline()\n","\tfout.close()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}